{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** \\_\\_\\_\\_\\_\n",
    "\n",
    "**EID:** \\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4487 - Tutorial 2: Text Document Classification with Naive Bayes\n",
    "\n",
    "In this tutorial you will classify text documents using Naive Bayes classifers.  We will be working with the dataset called \"20 Newsgroups\", which is a collection of 20,000 newsgroup posts organized into 20 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the 20 Newsgroups Dataset\n",
    "The dataset is called “20 Newsgroups”. Here is the official description, quoted from the [website](http://qwone.com/~jason/20Newsgroups/)\n",
    ">The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First we need to initialize Python.  Run the below cell.\n",
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Put the file \"20news-bydate_py3.pkz' into the same directory as this ipynb file. **Do not unzip the file**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract 4 classes ('alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space') from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# strip away headers/footers/quotes from the text\n",
    "removeset = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# only use 4 categories\n",
    "cats      = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# load the training and testing sets\n",
    "newsgroups_train = datasets.fetch_20newsgroups(subset='train',\n",
    "                           remove=removeset, categories=cats, data_home='./')\n",
    "newsgroups_test  = datasets.fetch_20newsgroups(subset='test', \n",
    "                           remove=removeset, categories=cats, data_home='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if we got all the data.  The training set should have 2034 documents, and the test set should have 1353 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"training set size:\", len(newsgroups_train.data))\n",
    "print(\"testing set size: \",  len(newsgroups_test.data))\n",
    "print(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the number examples in each class.  `newsgroups_train.target` is an array of class values (0 through 3), and `newsgroups_train.target[i]` is the class of the i-th document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"class counts\")\n",
    "for i in [0, 1, 2, 3]:\n",
    "    print(\"{:20s}: {}\".format(newsgroups_train.target_names[i], sum(newsgroups_train.target == i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show the documents.  `newsgroups_train.data` is a list of strings, and `newsgroups_train.data[i]` is the i-th document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [0, 1, 2 ,3]:\n",
    "    print(\"--- document {} (class={}) ---\".format(\n",
    "        i, newsgroups_train.target_names[newsgroups_train.target[i]]))\n",
    "    print(newsgroups_train.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** while you do the tutorial, it is okay to make additional code cells in the file.  This will allow you to avoid re-running code (like training a classifier, then testing a classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Features from Text Files\n",
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.\n",
    "\n",
    "Next, we will introduce two basic text representation methods: One-hot encoding, Bag of words, and TF-IDF. More feature vector extraction functions, please refer to https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding\n",
    "- Each word is coded with an index, which is represented by one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> John likes to watch movies. Mary likes too.\n",
    "\n",
    "> John also likes to watch football games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to represent the words in the above two sentences, you can encode the words as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> {\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\":6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode each word using one-hot method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    ">likes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    ">..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, this text representation method is impractical when the scale of corpus becomes large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "- The index value of a word in the vocabulary is linked to its frequency in the whole training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> John likes to watch movies. Mary likes too.  -->> [1, 2, 1, 1, 1, 0, 0, 0, 1, 1]\n",
    "\n",
    "> John also likes to watch football games.     -->> [1, 1, 1, 1, 0, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sklearn.feature_extraction.text.CountVectorizer** implement the `Bag of Words` method that converts a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using **scipy.sparse.coo_matrix** to save memory by only storing the non-zero parts of the feature vectors in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "corpus = ['This is the first document.',\n",
    "\t'This document is the second document.',\n",
    "\t'And this is the third one.',\n",
    "\t'Is this the first document?']\n",
    " \n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    " \n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word bag model, we can get the vector representation of this text.However, in the face of the diversity of text, each word has different weight to the content of text in practical application, so we introduce tf-idf model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF (Term Frequency)\n",
    "\n",
    "In the case of the term frequency $tf(t, d)$, the simplest choice is to use the raw count of a term in a document, i.e., the number of times that term $t$ occurs in document $d$. If we denote the raw count by $f_{t, d}$, then the simplest tf scheme is $tf(t,d) = f_{t, d}$. \n",
    "\n",
    "$tf_{t, d} = n_{t, d}/\\sum_kn_{t, d}$\n",
    "\n",
    "The numerator in the above formula is the number of occurrences of the word in the document $d$, and the denominator is the sum of the occurrences of all words in the document $d$.\n",
    "\n",
    "##### IDF (Inverse Document Frequency) \n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient): \n",
    "\n",
    "$idf(t ,D) = log\\frac{N}{|\\{ d\\in D:t \\in d \\}|}$\n",
    "\n",
    "with \n",
    "- $N$: total number of documents in the corpus $N=|D|$\n",
    "- $|\\{ d\\in D:t \\in d \\}|$: number of documents where the term $t$ appears. If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to  $1+|\\{ d\\in D:t \\in d \\}|$\n",
    "\n",
    "Then tf-idf is calculated as: \n",
    "$tfidf(t, d, D) = tf(t, d) * idf(t, D)$\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using **sklearn.feature_extraction.text.TfidfTransformer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vocabulary from the training data.  Then use **sklearn.feature_extraction.text.CountVectorizer** to build the document vectors for the training and testing sets.  You can decide how many words you want in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "# setup the document vectorizer\n",
    "# - use \"english\" as stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bernoulli Naive Bayes \n",
    "Learn a Bernoulli Naive Bayes model from the training set.  What is the prediction accuracy on the test set?  Try different parameters (alpha, max_features, etc) to get the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "## HINT\n",
    "# 1.  naive_bayes.BernoulliNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most informative words for each category?  Run the below code.\n",
    "\n",
    "Note: `model.coef_[i]` will index the scores for the i-th class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the word names\n",
    "fnames = asarray(cntvect.get_feature_names())\n",
    "for i,c in enumerate(newsgroups_train.target_names):\n",
    "    tmp = argsort(bmodel.coef_[i])[-10:]\n",
    "    print(\"class\", c)\n",
    "    for t in tmp:\n",
    "        print(\"    {:9s} ({:.5f})\".format(fnames[t], bmodel.coef_[i][t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multinomial Naive Bayes model\n",
    "Now learn a multinomial Naive Bayes model using the TF-IDF representation for the documents.  Again try different parameter values to improve the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HINT \n",
    "# 1. feature_extraction.text.TfidfTransformer(use_idf=True, norm= )\n",
    "# 2.  naive_bayes.MultinomialNB(alpha= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most informative features for Multinomial model? Run the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the word names\n",
    "fnames = asarray(cntvect.get_feature_names())\n",
    "for i,c in enumerate(newsgroups_train.target_names):\n",
    "    tmp = argsort(mmodel_tf.coef_[i])[-10:]\n",
    "    print(\"class\", c)\n",
    "    for t in tmp:\n",
    "        print(\"    {:9s} ({:.5f})\".format(fnames[t], mmodel_tf.coef_[i][t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the most informative words differ between the TF-IDF multinomial model and the Bernoulli model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **INSERT YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **INSERT YOUR ANSWER HERE**\n",
    "- the TF-IDF words are more unique, e.g., for religion.misc, {christians, jesus, god} for TF-IDF religion, compared {people, just, don} for Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Effect of smoothing\n",
    "The smoothing (regularization) parameter has a big effect on the performance.  Using the Multinomial TF-IDF models, make a plot of accuracy versus different values of alpha. For each alpha, you need to train a new model. Which alpha value yields the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effect of vocabulary size\n",
    "The vocabulary size also affects the accuracy.  Make another plot of accuracy versus vocabulary size.  Which vocabulary size yields the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
