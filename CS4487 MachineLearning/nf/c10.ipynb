{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39ff0f71594f717cc2eca5663830d759cb2ae65f"
   },
   "source": [
    "<center><h2> **Transfer Learning** </h2></center>\n",
    "<h3> Note </h3>\n",
    "<ul> \n",
    "    <li>Resnet18 pretrained with Imagenet </li>\n",
    "    <li> Images resized to 224, with resnet's normalization. Resize doesn't work with multiprocessing, so data loading couldn't be parallelized. </li>\n",
    "  </ul>\n",
    "<h3> Configurations</h3>\n",
    "1. Only last custom linear layer trained : 77.75 %\n",
    "2. All layers trained with Adam : ~ 95% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.3.1+cu92\n",
      "  Using cached https://download.pytorch.org/whl/cu92/torch-1.3.1%2Bcu92-cp36-cp36m-win_amd64.whl\n",
      "Collecting torchvision==0.4.2+cu92\n",
      "  Using cached https://download.pytorch.org/whl/cu92/torchvision-0.4.2%2Bcu92-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from torch==1.3.1+cu92) (1.15.4)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from torchvision==0.4.2+cu92) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\anaconda3\\lib\\site-packages (from torchvision==0.4.2+cu92) (5.4.1)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Found existing installation: torch 1.3.1\n",
      "    Uninstalling torch-1.3.1:\n",
      "      Successfully uninstalled torch-1.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe and convert-onnx-to-caffe2.exe are installed in 'C:\\Users\\zhlow2\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\zhlow2\\\\appdata\\\\roaming\\\\python\\\\python36\\\\site-packages\\\\~orch\\\\lib\\\\c10.dll'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.3.1+cu92 torchvision==0.4.2+cu92 -f https://download.pytorch.org/whl/torch_stable.html --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "7e7300d91e3f65ec6b47d5dc137d0fd6d7ff6bb8"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6ba344e0830d9b3b85797f170c47d0719a8e7342"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x cifar-10-batches-py/\n",
      "x cifar-10-batches-py/data_batch_4\n",
      "x cifar-10-batches-py/readme.html\n",
      "x cifar-10-batches-py/test_batch\n",
      "x cifar-10-batches-py/data_batch_3\n",
      "x cifar-10-batches-py/batches.meta\n",
      "x cifar-10-batches-py/data_batch_2\n",
      "x cifar-10-batches-py/data_batch_5\n",
      "x cifar-10-batches-py/data_batch_1\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf ./cifar10-python/cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "6a5ecafccb7fa98ccd94044c1d6d2f92f4e300a7"
   },
   "outputs": [],
   "source": [
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='.', train=True, download=False, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='.', train=False, download=False, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file checkpoint already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.get_terminal_size().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n",
      "epoch: 214\n",
      "\n",
      "Epoch: 214\n",
      "acc: 86.54 best: 86.87\n",
      "==> Resuming from checkpoint..\n",
      "epoch: 214\n",
      "\n",
      "Epoch: 214\n",
      "acc: 86.54 best: 86.87\n",
      "epoch: 215\n",
      "\n",
      "Epoch: 215\n",
      "acc: 86.62 best: 86.87\n",
      "epoch: 216\n",
      "\n",
      "Epoch: 216\n",
      "acc: 86.58 best: 86.87\n",
      "epoch: 217\n",
      "\n",
      "Epoch: 217\n",
      "acc: 86.46 best: 86.87\n",
      "epoch: 218\n",
      "\n",
      "Epoch: 218\n",
      "acc: 86.91 best: 86.87\n",
      "Saving..\n",
      "epoch: 219\n",
      "\n",
      "Epoch: 219\n",
      "acc: 86.54 best: 86.91\n",
      "epoch: 220\n",
      "\n",
      "Epoch: 220\n",
      "acc: 86.5 best: 86.91\n",
      "epoch: 221\n",
      "\n",
      "Epoch: 221\n",
      "acc: 86.54 best: 86.91\n",
      "epoch: 222\n",
      "\n",
      "Epoch: 222\n",
      "acc: 86.73 best: 86.91\n",
      "epoch: 223\n",
      "\n",
      "Epoch: 223\n",
      "acc: 86.88 best: 86.91\n",
      "epoch: 224\n",
      "\n",
      "Epoch: 224\n",
      "acc: 86.54 best: 86.91\n",
      "epoch: 225\n",
      "\n",
      "Epoch: 225\n",
      "acc: 86.88 best: 86.91\n",
      "epoch: 226\n",
      "\n",
      "Epoch: 226\n",
      "acc: 86.53 best: 86.91\n",
      "epoch: 227\n",
      "\n",
      "Epoch: 227\n",
      "acc: 86.67 best: 86.91\n",
      "epoch: 228\n",
      "\n",
      "Epoch: 228\n",
      "acc: 86.62 best: 86.91\n",
      "epoch: 229\n",
      "\n",
      "Epoch: 229\n",
      "acc: 86.89 best: 86.91\n",
      "epoch: 230\n",
      "\n",
      "Epoch: 230\n",
      "acc: 86.87 best: 86.91\n",
      "epoch: 231\n",
      "\n",
      "Epoch: 231\n",
      "acc: 86.66 best: 86.91\n",
      "epoch: 232\n",
      "\n",
      "Epoch: 232\n",
      "acc: 86.86 best: 86.91\n",
      "epoch: 233\n",
      "\n",
      "Epoch: 233\n",
      "acc: 86.43 best: 86.91\n",
      "epoch: 234\n",
      "\n",
      "Epoch: 234\n",
      "acc: 86.97 best: 86.91\n",
      "Saving..\n",
      "epoch: 235\n",
      "\n",
      "Epoch: 235\n",
      "acc: 86.73 best: 86.97\n",
      "epoch: 236\n",
      "\n",
      "Epoch: 236\n",
      "acc: 86.97 best: 86.97\n",
      "epoch: 237\n",
      "\n",
      "Epoch: 237\n",
      "acc: 87.02 best: 86.97\n",
      "Saving..\n",
      "epoch: 238\n",
      "\n",
      "Epoch: 238\n",
      "acc: 86.9 best: 87.02\n",
      "epoch: 239\n",
      "\n",
      "Epoch: 239\n",
      "acc: 86.88 best: 87.02\n",
      "epoch: 240\n",
      "\n",
      "Epoch: 240\n",
      "acc: 86.43 best: 87.02\n",
      "epoch: 241\n",
      "\n",
      "Epoch: 241\n",
      "acc: 86.76 best: 87.02\n",
      "epoch: 242\n",
      "\n",
      "Epoch: 242\n",
      "acc: 86.86 best: 87.02\n",
      "epoch: 243\n",
      "\n",
      "Epoch: 243\n",
      "acc: 87.04 best: 87.02\n",
      "Saving..\n",
      "epoch: 244\n",
      "\n",
      "Epoch: 244\n",
      "acc: 86.61 best: 87.04\n",
      "epoch: 245\n",
      "\n",
      "Epoch: 245\n",
      "acc: 86.55 best: 87.04\n",
      "epoch: 246\n",
      "\n",
      "Epoch: 246\n",
      "acc: 86.91 best: 87.04\n",
      "epoch: 247\n",
      "\n",
      "Epoch: 247\n",
      "acc: 86.72 best: 87.04\n",
      "epoch: 248\n",
      "\n",
      "Epoch: 248\n",
      "acc: 86.97 best: 87.04\n",
      "epoch: 249\n",
      "\n",
      "Epoch: 249\n",
      "acc: 86.86 best: 87.04\n",
      "epoch: 250\n",
      "\n",
      "Epoch: 250\n",
      "acc: 87.0 best: 87.04\n",
      "epoch: 251\n",
      "\n",
      "Epoch: 251\n",
      "acc: 86.95 best: 87.04\n",
      "epoch: 252\n",
      "\n",
      "Epoch: 252\n",
      "acc: 86.87 best: 87.04\n",
      "epoch: 253\n",
      "\n",
      "Epoch: 253\n",
      "acc: 86.77 best: 87.04\n",
      "epoch: 254\n",
      "\n",
      "Epoch: 254\n",
      "acc: 87.0 best: 87.04\n",
      "epoch: 255\n",
      "\n",
      "Epoch: 255\n",
      "acc: 87.22 best: 87.04\n",
      "Saving..\n",
      "epoch: 256\n",
      "\n",
      "Epoch: 256\n",
      "acc: 86.97 best: 87.22\n",
      "epoch: 257\n",
      "\n",
      "Epoch: 257\n",
      "acc: 86.63 best: 87.22\n",
      "epoch: 258\n",
      "\n",
      "Epoch: 258\n",
      "acc: 87.08 best: 87.22\n",
      "epoch: 259\n",
      "\n",
      "Epoch: 259\n",
      "acc: 87.21 best: 87.22\n",
      "epoch: 260\n",
      "\n",
      "Epoch: 260\n",
      "acc: 87.06 best: 87.22\n",
      "epoch: 261\n",
      "\n",
      "Epoch: 261\n",
      "acc: 86.83 best: 87.22\n",
      "epoch: 262\n",
      "\n",
      "Epoch: 262\n",
      "acc: 86.71 best: 87.22\n",
      "epoch: 263\n",
      "\n",
      "Epoch: 263\n",
      "acc: 86.94 best: 87.22\n",
      "==> Resuming from checkpoint..\n",
      "epoch: 255\n",
      "\n",
      "Epoch: 255\n",
      "acc: 87.1 best: 87.22\n",
      "epoch: 256\n",
      "\n",
      "Epoch: 256\n",
      "acc: 87.23 best: 87.22\n",
      "Saving..\n",
      "epoch: 257\n",
      "\n",
      "Epoch: 257\n",
      "acc: 87.49 best: 87.23\n",
      "Saving..\n",
      "epoch: 258\n",
      "\n",
      "Epoch: 258\n",
      "acc: 87.21 best: 87.49\n",
      "epoch: 259\n",
      "\n",
      "Epoch: 259\n",
      "acc: 87.13 best: 87.49\n",
      "epoch: 260\n",
      "\n",
      "Epoch: 260\n",
      "acc: 87.02 best: 87.49\n",
      "epoch: 261\n",
      "\n",
      "Epoch: 261\n",
      "acc: 87.27 best: 87.49\n",
      "epoch: 262\n",
      "\n",
      "Epoch: 262\n",
      "acc: 87.27 best: 87.49\n",
      "epoch: 263\n",
      "\n",
      "Epoch: 263\n",
      "acc: 87.3 best: 87.49\n",
      "epoch: 264\n",
      "\n",
      "Epoch: 264\n",
      "acc: 87.11 best: 87.49\n",
      "epoch: 265\n",
      "\n",
      "Epoch: 265\n",
      "acc: 87.12 best: 87.49\n",
      "epoch: 266\n",
      "\n",
      "Epoch: 266\n",
      "acc: 87.51 best: 87.49\n",
      "Saving..\n",
      "epoch: 267\n",
      "\n",
      "Epoch: 267\n",
      "acc: 87.4 best: 87.51\n",
      "epoch: 268\n",
      "\n",
      "Epoch: 268\n",
      "acc: 87.16 best: 87.51\n",
      "epoch: 269\n",
      "\n",
      "Epoch: 269\n",
      "acc: 87.12 best: 87.51\n",
      "epoch: 270\n",
      "\n",
      "Epoch: 270\n",
      "acc: 87.27 best: 87.51\n",
      "epoch: 271\n",
      "\n",
      "Epoch: 271\n",
      "acc: 87.22 best: 87.51\n",
      "epoch: 272\n",
      "\n",
      "Epoch: 272\n",
      "acc: 87.08 best: 87.51\n",
      "epoch: 273\n",
      "\n",
      "Epoch: 273\n",
      "acc: 87.1 best: 87.51\n",
      "epoch: 274\n",
      "\n",
      "Epoch: 274\n",
      "acc: 87.32 best: 87.51\n",
      "epoch: 275\n",
      "\n",
      "Epoch: 275\n",
      "acc: 87.3 best: 87.51\n",
      "epoch: 276\n",
      "\n",
      "Epoch: 276\n",
      "acc: 87.34 best: 87.51\n",
      "epoch: 277\n",
      "\n",
      "Epoch: 277\n",
      "acc: 86.95 best: 87.51\n",
      "epoch: 278\n",
      "\n",
      "Epoch: 278\n",
      "acc: 86.97 best: 87.51\n",
      "epoch: 279\n",
      "\n",
      "Epoch: 279\n",
      "acc: 86.81 best: 87.51\n",
      "epoch: 280\n",
      "\n",
      "Epoch: 280\n",
      "acc: 87.18 best: 87.51\n",
      "epoch: 281\n",
      "\n",
      "Epoch: 281\n",
      "acc: 86.89 best: 87.51\n",
      "epoch: 282\n",
      "\n",
      "Epoch: 282\n",
      "acc: 87.03 best: 87.51\n",
      "epoch: 283\n",
      "\n",
      "Epoch: 283\n",
      "acc: 87.15 best: 87.51\n",
      "epoch: 284\n",
      "\n",
      "Epoch: 284\n",
      "acc: 87.02 best: 87.51\n",
      "epoch: 285\n",
      "\n",
      "Epoch: 285\n",
      "acc: 87.0 best: 87.51\n",
      "epoch: 286\n",
      "\n",
      "Epoch: 286\n",
      "acc: 87.12 best: 87.51\n",
      "epoch: 287\n",
      "\n",
      "Epoch: 287\n",
      "acc: 86.96 best: 87.51\n",
      "epoch: 288\n",
      "\n",
      "Epoch: 288\n",
      "acc: 86.95 best: 87.51\n",
      "epoch: 289\n",
      "\n",
      "Epoch: 289\n",
      "acc: 86.99 best: 87.51\n",
      "epoch: 290\n",
      "\n",
      "Epoch: 290\n",
      "acc: 86.78 best: 87.51\n",
      "epoch: 291\n",
      "\n",
      "Epoch: 291\n",
      "acc: 86.85 best: 87.51\n",
      "epoch: 292\n",
      "\n",
      "Epoch: 292\n",
      "acc: 87.23 best: 87.51\n",
      "epoch: 293\n",
      "\n",
      "Epoch: 293\n",
      "acc: 86.96 best: 87.51\n",
      "epoch: 294\n",
      "\n",
      "Epoch: 294\n",
      "acc: 86.99 best: 87.51\n",
      "epoch: 295\n",
      "\n",
      "Epoch: 295\n",
      "acc: 87.14 best: 87.51\n",
      "epoch: 296\n",
      "\n",
      "Epoch: 296\n",
      "acc: 86.83 best: 87.51\n",
      "epoch: 297\n",
      "\n",
      "Epoch: 297\n",
      "acc: 87.02 best: 87.51\n",
      "epoch: 298\n",
      "\n",
      "Epoch: 298\n",
      "acc: 87.02 best: 87.51\n",
      "epoch: 299\n",
      "\n",
      "Epoch: 299\n",
      "acc: 86.83 best: 87.51\n",
      "epoch: 300\n",
      "\n",
      "Epoch: 300\n",
      "acc: 87.19 best: 87.51\n",
      "epoch: 301\n",
      "\n",
      "Epoch: 301\n",
      "acc: 87.17 best: 87.51\n",
      "epoch: 302\n",
      "\n",
      "Epoch: 302\n",
      "acc: 87.14 best: 87.51\n",
      "epoch: 303\n",
      "\n",
      "Epoch: 303\n",
      "acc: 87.03 best: 87.51\n",
      "epoch: 304\n",
      "\n",
      "Epoch: 304\n",
      "acc: 87.23 best: 87.51\n",
      "epoch: 305\n",
      "\n",
      "Epoch: 305\n",
      "acc: 87.17 best: 87.51\n",
      "epoch: 306\n",
      "\n",
      "Epoch: 306\n",
      "acc: 87.21 best: 87.51\n",
      "epoch: 307\n",
      "\n",
      "Epoch: 307\n",
      "acc: 87.3 best: 87.51\n",
      "epoch: 308\n",
      "\n",
      "Epoch: 308\n",
      "acc: 87.25 best: 87.51\n",
      "epoch: 309\n",
      "\n",
      "Epoch: 309\n",
      "acc: 87.08 best: 87.51\n",
      "epoch: 310\n",
      "\n",
      "Epoch: 310\n",
      "acc: 87.14 best: 87.51\n",
      "epoch: 311\n",
      "\n",
      "Epoch: 311\n",
      "acc: 87.05 best: 87.51\n",
      "epoch: 312\n",
      "\n",
      "Epoch: 312\n",
      "acc: 87.31 best: 87.51\n",
      "epoch: 313\n",
      "\n",
      "Epoch: 313\n",
      "acc: 87.14 best: 87.51\n",
      "epoch: 314\n",
      "\n",
      "Epoch: 314\n",
      "acc: 87.2 best: 87.51\n",
      "epoch: 315\n",
      "\n",
      "Epoch: 315\n",
      "acc: 87.27 best: 87.51\n",
      "epoch: 316\n",
      "\n",
      "Epoch: 316\n",
      "acc: 87.46 best: 87.51\n",
      "epoch: 317\n",
      "\n",
      "Epoch: 317\n",
      "acc: 87.07 best: 87.51\n",
      "epoch: 318\n",
      "\n",
      "Epoch: 318\n",
      "acc: 86.85 best: 87.51\n",
      "epoch: 319\n",
      "\n",
      "Epoch: 319\n",
      "acc: 87.09 best: 87.51\n",
      "epoch: 320\n",
      "\n",
      "Epoch: 320\n",
      "acc: 87.11 best: 87.51\n",
      "epoch: 321\n",
      "\n",
      "Epoch: 321\n",
      "acc: 87.27 best: 87.51\n",
      "epoch: 322\n",
      "\n",
      "Epoch: 322\n",
      "acc: 87.36 best: 87.51\n",
      "epoch: 323\n",
      "\n",
      "Epoch: 323\n",
      "acc: 87.39 best: 87.51\n",
      "epoch: 324\n",
      "\n",
      "Epoch: 324\n",
      "acc: 87.39 best: 87.51\n",
      "epoch: 325\n",
      "\n",
      "Epoch: 325\n",
      "acc: 87.19 best: 87.51\n",
      "epoch: 326\n",
      "\n",
      "Epoch: 326\n",
      "acc: 87.22 best: 87.51\n",
      "epoch: 327\n",
      "\n",
      "Epoch: 327\n",
      "acc: 87.0 best: 87.51\n",
      "epoch: 328\n",
      "\n",
      "Epoch: 328\n",
      "acc: 87.39 best: 87.51\n",
      "epoch: 329\n",
      "\n",
      "Epoch: 329\n",
      "acc: 87.3 best: 87.51\n",
      "epoch: 330\n",
      "\n",
      "Epoch: 330\n",
      "acc: 87.07 best: 87.51\n",
      "epoch: 331\n",
      "\n",
      "Epoch: 331\n",
      "acc: 87.29 best: 87.51\n",
      "epoch: 332\n",
      "\n",
      "Epoch: 332\n",
      "acc: 87.14 best: 87.51\n",
      "epoch: 333\n",
      "\n",
      "Epoch: 333\n",
      "acc: 87.24 best: 87.51\n",
      "epoch: 334\n",
      "\n",
      "Epoch: 334\n",
      "acc: 87.33 best: 87.51\n",
      "epoch: 335\n",
      "\n",
      "Epoch: 335\n",
      "acc: 87.5 best: 87.51\n",
      "epoch: 336\n",
      "\n",
      "Epoch: 336\n",
      "acc: 87.6 best: 87.51\n",
      "Saving..\n",
      "epoch: 337\n",
      "\n",
      "Epoch: 337\n",
      "acc: 87.45 best: 87.6\n",
      "epoch: 338\n",
      "\n",
      "Epoch: 338\n",
      "acc: 87.28 best: 87.6\n",
      "epoch: 339\n",
      "\n",
      "Epoch: 339\n",
      "acc: 87.33 best: 87.6\n",
      "epoch: 340\n",
      "\n",
      "Epoch: 340\n",
      "acc: 87.28 best: 87.6\n",
      "epoch: 341\n",
      "\n",
      "Epoch: 341\n",
      "acc: 87.41 best: 87.6\n",
      "epoch: 342\n",
      "\n",
      "Epoch: 342\n",
      "acc: 87.26 best: 87.6\n",
      "epoch: 343\n",
      "\n",
      "Epoch: 343\n",
      "acc: 87.14 best: 87.6\n",
      "epoch: 344\n",
      "\n",
      "Epoch: 344\n",
      "acc: 87.43 best: 87.6\n",
      "epoch: 345\n",
      "\n",
      "Epoch: 345\n",
      "acc: 87.08 best: 87.6\n",
      "epoch: 346\n",
      "\n",
      "Epoch: 346\n",
      "acc: 87.16 best: 87.6\n",
      "epoch: 347\n",
      "\n",
      "Epoch: 347\n",
      "acc: 87.16 best: 87.6\n",
      "epoch: 348\n",
      "\n",
      "Epoch: 348\n",
      "acc: 87.5 best: 87.6\n",
      "epoch: 349\n",
      "\n",
      "Epoch: 349\n",
      "acc: 86.98 best: 87.6\n",
      "epoch: 350\n",
      "\n",
      "Epoch: 350\n",
      "acc: 87.42 best: 87.6\n",
      "epoch: 351\n",
      "\n",
      "Epoch: 351\n",
      "acc: 87.08 best: 87.6\n",
      "epoch: 352\n",
      "\n",
      "Epoch: 352\n",
      "acc: 87.29 best: 87.6\n",
      "epoch: 353\n",
      "\n",
      "Epoch: 353\n",
      "acc: 87.12 best: 87.6\n",
      "epoch: 354\n",
      "\n",
      "Epoch: 354\n",
      "acc: 87.3 best: 87.6\n"
     ]
    }
   ],
   "source": [
    "'''EfficientNet in PyTorch.\n",
    "Paper: \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\".\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "term_width =os.get_terminal_size().columns\n",
    "term_width = int(term_width)\n",
    "\n",
    "\n",
    "TOTAL_BAR_LENGTH = 65.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "def main(resume,lr,epochAdd):\n",
    "    global best_acc, start_epoch, term_width, TOTAL_BAR_LENGTH,last_time ,begin_time,device\n",
    "    \n",
    "\n",
    "\n",
    "    def get_mean_and_std(dataset):\n",
    "        '''Compute the mean and std value of dataset.'''\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "        mean = torch.zeros(3)\n",
    "        std = torch.zeros(3)\n",
    "        print('==> Computing mean and std..')\n",
    "        for inputs, targets in dataloader:\n",
    "            for i in range(3):\n",
    "                mean[i] += inputs[:,i,:,:].mean()\n",
    "                std[i] += inputs[:,i,:,:].std()\n",
    "        mean.div_(len(dataset))\n",
    "        std.div_(len(dataset))\n",
    "        return mean, std\n",
    "\n",
    "    def init_params(net):\n",
    "        '''Init layer parameters.'''\n",
    "        for m in net.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal(m.weight, mode='fan_out')\n",
    "                if m.bias:\n",
    "                    init.constant(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant(m.weight, 1)\n",
    "                init.constant(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal(m.weight, std=1e-3)\n",
    "                if m.bias:\n",
    "                    init.constant(m.bias, 0)\n",
    "\n",
    "    def progress_bar(current, total, msg=None):\n",
    "        global last_time, begin_time\n",
    "        if current == 0:\n",
    "            begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "        cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "        rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "        sys.stdout.write(' [')\n",
    "        for i in range(cur_len):\n",
    "            sys.stdout.write('=')\n",
    "        sys.stdout.write('>')\n",
    "        for i in range(rest_len):\n",
    "            sys.stdout.write('.')\n",
    "        sys.stdout.write(']')\n",
    "\n",
    "        cur_time = time.time()\n",
    "        step_time = cur_time - last_time\n",
    "        last_time = cur_time\n",
    "        tot_time = cur_time - begin_time\n",
    "\n",
    "        L = []\n",
    "        L.append('  Step: %s' % format_time(step_time))\n",
    "        L.append(' | Tot: %s' % format_time(tot_time))\n",
    "        if msg:\n",
    "            L.append(' | ' + msg)\n",
    "\n",
    "        msg = ''.join(L)\n",
    "        sys.stdout.write(msg)\n",
    "        for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "            sys.stdout.write(' ')\n",
    "\n",
    "        # Go back to the center of the bar.\n",
    "        for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "            sys.stdout.write('\\b')\n",
    "        sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "        if current < total-1:\n",
    "            sys.stdout.write('\\r')\n",
    "        else:\n",
    "            sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def format_time(seconds):\n",
    "        days = int(seconds / 3600/24)\n",
    "        seconds = seconds - days*3600*24\n",
    "        hours = int(seconds / 3600)\n",
    "        seconds = seconds - hours*3600\n",
    "        minutes = int(seconds / 60)\n",
    "        seconds = seconds - minutes*60\n",
    "        secondsf = int(seconds)\n",
    "        seconds = seconds - secondsf\n",
    "        millis = int(seconds*1000)\n",
    "\n",
    "        f = ''\n",
    "        i = 1\n",
    "        if days > 0:\n",
    "            f += str(days) + 'D'\n",
    "            i += 1\n",
    "        if hours > 0 and i <= 2:\n",
    "            f += str(hours) + 'h'\n",
    "            i += 1\n",
    "        if minutes > 0 and i <= 2:\n",
    "            f += str(minutes) + 'm'\n",
    "            i += 1\n",
    "        if secondsf > 0 and i <= 2:\n",
    "            f += str(secondsf) + 's'\n",
    "            i += 1\n",
    "        if millis > 0 and i <= 2:\n",
    "            f += str(millis) + 'ms'\n",
    "            i += 1\n",
    "        if f == '':\n",
    "            f = '0ms'\n",
    "        return f\n",
    "    \n",
    "    class Block(nn.Module):\n",
    "        '''expand + depthwise + pointwise + squeeze-excitation'''\n",
    "\n",
    "        def __init__(self, in_planes, out_planes, expansion, stride):\n",
    "            super(Block, self).__init__()\n",
    "            self.stride = stride\n",
    "\n",
    "            planes = expansion * in_planes\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                                   stride=stride, padding=1, groups=planes, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(planes)\n",
    "            self.conv3 = nn.Conv2d(\n",
    "                planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "            self.shortcut = nn.Sequential()\n",
    "            if stride == 1 and in_planes != out_planes:\n",
    "                self.shortcut = nn.Sequential(\n",
    "                    nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
    "                              stride=1, padding=0, bias=False),\n",
    "                    nn.BatchNorm2d(out_planes),\n",
    "                )\n",
    "\n",
    "            # SE layers\n",
    "            self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
    "            self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = F.relu(self.bn1(self.conv1(x)))\n",
    "            out = F.relu(self.bn2(self.conv2(out)))\n",
    "            out = self.bn3(self.conv3(out))\n",
    "            shortcut = self.shortcut(x) if self.stride == 1 else out\n",
    "            # Squeeze-Excitation\n",
    "            w = F.avg_pool2d(out, out.size(2))\n",
    "            w = F.relu(self.fc1(w))\n",
    "            w = self.fc2(w).sigmoid()\n",
    "            out = out * w + shortcut\n",
    "            return out\n",
    "\n",
    "\n",
    "    class EfficientNet(nn.Module):\n",
    "        def __init__(self, cfg, num_classes=10):\n",
    "            super(EfficientNet, self).__init__()\n",
    "            self.cfg = cfg\n",
    "            self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
    "                                   stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.layers = self._make_layers(in_planes=32)\n",
    "            self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
    "\n",
    "        def _make_layers(self, in_planes):\n",
    "            layers = []\n",
    "            for expansion, out_planes, num_blocks, stride in self.cfg:\n",
    "                strides = [stride] + [1]*(num_blocks-1)\n",
    "                for stride in strides:\n",
    "                    layers.append(Block(in_planes, out_planes, expansion, stride))\n",
    "                    in_planes = out_planes\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = F.relu(self.bn1(self.conv1(x)))\n",
    "            out = self.layers(out)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.linear(out)\n",
    "            return out\n",
    "\n",
    "\n",
    "    def EfficientNetB0():\n",
    "        # (expansion, out_planes, num_blocks, stride)\n",
    "        cfg = [(1,  16, 1, 2),\n",
    "               (6,  24, 2, 1),\n",
    "               (6,  40, 2, 2),\n",
    "               (6,  80, 3, 2),\n",
    "               (6, 112, 3, 1),\n",
    "               (6, 192, 4, 2),\n",
    "               (6, 320, 1, 2)]\n",
    "        return EfficientNet(cfg)\n",
    "    net = EfficientNetB0()\n",
    "    net = net.to(device)\n",
    "    if device == 'cuda':\n",
    "        net = torch.nn.DataParallel(net)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    if resume:\n",
    "        # Load checkpoint.\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "        checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        best_acc = 0  # best test accuracy\n",
    "        start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    # Training\n",
    "    def train(epoch):\n",
    "        print('\\nEpoch: %d' % epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#             progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#                 % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    def test(epoch):\n",
    "        global best_acc\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "#                 progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "#                     % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "        # Save checkpoint.\n",
    "        acc = 100.*correct/total\n",
    "        print('acc: '+str(acc) + ' last best: '+ str(best_acc))\n",
    "        if acc > best_acc:\n",
    "            print('Saving..')\n",
    "            state = {\n",
    "                'net': net.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            torch.save(state, './checkpoint/ckpt.pth')\n",
    "            best_acc = acc\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch+epochAdd):\n",
    "        print('epoch: '+str(epoch))\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "lrate= [0.001,0.001,0.0005]\n",
    "epochAdd=[1,50,100]\n",
    "nettt = []\n",
    "for i in range(3):\n",
    "    nettt.append(main(True,lrate[i],epochAdd[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../input/temptraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba17e22f371ef2ac80891ac58f275377e5d03d0b"
   },
   "outputs": [],
   "source": [
    "# class Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         base = models.resnet18(pretrained=True)\n",
    "#         self.base = nn.Sequential(*list(base.children())[:-1])\n",
    "#         in_features = base.fc.in_features\n",
    "#         self.drop = nn.Dropout()\n",
    "#         self.final = nn.Linear(in_features,10)\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         x = self.base(x)\n",
    "#         x = self.drop(x.view(-1,self.final.in_features))\n",
    "#         return self.final(x)\n",
    "    \n",
    "model = EfficientNetB0().cuda()\n",
    "[x for x,y in model.named_children()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58e17ec9fe8d087c72913c959fa8a57b82099ff1"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "param_groups = [\n",
    "    {'params':model.base.parameters(),'lr':.0001},\n",
    "    {'params':model.final.parameters(),'lr':.001}\n",
    "]\n",
    "optimizer = Adam(param_groups)\n",
    "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "states = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c75daaac59a9c34308bc90f0131860596eb5b237"
   },
   "source": [
    "<h2> Training the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25b88ad5eadaa85a2c5b87039cf5914ff9857b31"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best_val_acc = -1000\n",
    "best_val_model = None\n",
    "for epoch in range(10):  \n",
    "    model.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(),labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        out = torch.argmax(outputs.detach(),dim=1)\n",
    "        assert out.shape==labels.shape\n",
    "        running_acc += (labels==out).sum().item()\n",
    "    print(f\"Train loss {epoch+1}: {running_loss/len(trainset)},Train Acc:{running_acc*100/len(trainset)}%\")\n",
    "    \n",
    "    correct = 0\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        for inputs,labels in valloader:\n",
    "            out = model(inputs.cuda()).cpu()\n",
    "            out = torch.argmax(out,dim=1)\n",
    "            acc = (out==labels).sum().item()\n",
    "            correct += acc\n",
    "    print(f\"Val accuracy:{correct*100/len(valset)}%\")\n",
    "    if correct>best_val_acc:\n",
    "        best_val_acc = correct\n",
    "        best_val_model = deepcopy(model.state_dict())\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "print('Finished Training')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "151d3e2d1c3e0be2934332e6c8c1393f18cab01c"
   },
   "source": [
    "<h2> Testing the model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "221a97d00fee49753d0e982acb6e8e3a42058f06"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "correct = 0\n",
    "model.load_state_dict(best_val_model)\n",
    "model.train(False)\n",
    "with torch.no_grad():\n",
    "    for inputs,labels in testloader:\n",
    "        out = model(inputs.cuda()).cpu()\n",
    "        out = torch.argmax(out,dim=1)\n",
    "        acc = (out==labels).sum().item()\n",
    "        \n",
    "        correct += acc\n",
    "print(f\"Test accuracy: {correct*100/len(testset)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_val_model,'./wtfIsWrongWithKaggleNotebook') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d10c52e79a9ee8591097704dcf00aa21e06444d"
   },
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "pp = np.load('/kaggle/input/test-data/y_test.npy')\n",
    "pp_torch = transform_test(pp[0])\n",
    "# nettt[len(nettt)-1].eval()\n",
    "# with torch.no_grad():\n",
    "#     input = pp_torch.to(device)\n",
    "#     outputs = nettt[len(nettt)-1](inputs)\n",
    "#     _, predicted = outputs.max(1)\n",
    "\n",
    "\n",
    "model.load_state_dict(best_val_model)\n",
    "model.train(False)\n",
    "with torch.no_grad():\n",
    "#     for inputs,labels in testloader:\n",
    "    out = model(pp_torch.cuda()).cpu()\n",
    "    out = torch.argmax(out,dim=1)\n",
    "#     acc = (out==labels).sum().item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../input/test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
